{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zdTTmRX8hxoe",
        "1lAEKNygpVEZ",
        "4k2UoxADQIYm",
        "eQw_0tpYQW8U",
        "Flg7Vide5-hw",
        "nGFNfGniRR3h",
        "5k91HrovrqB_"
      ],
      "authorship_tag": "ABX9TyPgo1qCjdl1ti4Tms+S4cDI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mb26-code/ter-rel-sem/blob/main/Extraction_relations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explication de l'algo\n",
        "Je crée une classe `TokenAnnotation` pour traiter chaque token (mot). Cette classe est pour but de stocker des infos du token lui-même, comme :\n",
        "- text : texte brut\n",
        "- lemma : texte après traitement\n",
        "- pos : nature du mot (adj, adv, nom)\n",
        "- dep : label de dépendence, pour définir sa relation avec un autre token (head) dans la phrase, comme `nsubj` --> nominal subject\n",
        "- head : indice de `head` de ce token. Càd ce token est le dépendent de head\n",
        "\n",
        "Ultérieurement, on va se baser sur cette `dep` pour filtrer quelles dépendences qu'on prendra pour tester avec l'API JdM, au lieu de brute-force tous les cas inutiles.\n",
        "\n",
        "Par exemple, prenons cette phrase :\n",
        "**Le bouillon contient traditionnellement un oignon piqué de clous de girofle, l'ail est déconseillé.**\n",
        "\n",
        "Après l'extraction, `girofle` a son head `clou` et sa dep `nmod`.\n",
        "```\n",
        "head | lemma | dep\n",
        "('clou', 'girofle', 'nmod')\n",
        "```\n",
        "Vérifions avec JdM : https://jdm-api.demo.lirmm.fr/v0/relations/from/clou/to/girofle\n",
        "\n",
        "On aura 2 relations :\n",
        "\n",
        "- Type 0 : r_associated\n",
        "- Type 8 : r_hypo\n",
        "\n",
        "Donc, on sauvegardera dans la BdD cette paire et ses relations, à filtrer par le poids `w` si nécessaire.\n",
        "\n",
        "À consulter les relations types : https://jdm-api.demo.lirmm.fr/v0/relations_types#\n",
        "\n"
      ],
      "metadata": {
        "id": "oK2_nix4F06c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filtrer par Word2Vec avant d'envoyer ces paires à JdM\n",
        "\n",
        "Word2Vec :\n",
        "1. Principe de base\n",
        "Word2Vec encode chaque mot sous forme de vecteur en se basant sur ses contexte d’apparition (les mots voisins dans un corpus), pas sur sa forme ou sa morphologie.\n",
        "2. Entraînement\n",
        "-\tSkip-Gram : pour un mot central w, le modèle apprend à prédire les mots qui l’entourent.\n",
        "-\tCBOW : pour un ensemble de mots de contexte, il prédit le mot central.\n",
        "Le résultat : deux mots qui partagent des contextes similaires (ex. “girofle” et “épice”) auront des vecteurs proches.\n",
        "3. Centroïde de domaine\n",
        "On choisit quelques pivots représentatifs du domaine (ex. “cuisine”, “recette”, “ingrédient”) et on fait la moyenne de leurs vecteurs pour obtenir un vecteur-prototype du domaine gastronomie.\n",
        "\n",
        "**--> Donc, on va créer un vecteur à partir des mots qu'on pense qu'ils sont liés à la gastronomie**"
      ],
      "metadata": {
        "id": "Flg7Vide5-hw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Envoyer les paires récupérées ci-dessus et vérifier avec JdM\n",
        "\n",
        "`https://jdm-api.demo.lirmm.fr/v0/relations/from/[head]/to/[lemma]`\n",
        "\n",
        "https://jdm-api.demo.lirmm.fr/v0/relations/from/clou/to/girofle\n",
        "\n",
        "**Puis, sauvegarder sous format .csv pour analyser.**\n"
      ],
      "metadata": {
        "id": "nGFNfGniRR3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Solution complète à partir de cette cellule"
      ],
      "metadata": {
        "id": "Bia5L5L6-ZKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Clean problematic packages\n",
        "!pip uninstall -y numpy spacy gensim\n",
        "\n",
        "# STEP 2: Reinstall only compatible versions\n",
        "!pip install numpy==1.26.4 --no-cache-dir\n",
        "!pip install spacy gensim --no-cache-dir\n",
        "!wget -c \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.vec.gz\" -O \"cc.fr.300.vec.gz\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "EsC6b4rmBll_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download fr_core_news_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEZ68JorCIQY",
        "outputId": "f2a740ba-3e18-4e16-c664-6406bbc8a3cb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fr-core-news-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_lg-3.8.0/fr_core_news_lg-3.8.0-py3-none-any.whl (571.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.8/571.8 MB\u001b[0m \u001b[31m827.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "my_local_drive='/content/gdrive/My Drive/TER/'\n",
        "sys.path.append(my_local_drive)\n",
        "\n",
        "PATH_RELATIONS_TYPES = f\"{my_local_drive}relations_types.json\" # Liste des relations\n",
        "PATH_PAIRS = f\"{my_local_drive}pairs.txt\" # Paires récupérées\n",
        "PATH_FILTERED_PAIRS = f\"{my_local_drive}filtered_pairs.txt\" # Paires filtrées par Word2Vec\n",
        "PATH_UNDER_PAIRS = f\"{my_local_drive}under_threshold_pairs.txt\"\n",
        "PATH_RELATIONS_RESULTS = f\"{my_local_drive}relations_results.csv\" # Résultats retournés de JdM\n",
        "\n",
        "CURR_FOLDER = \"wikipedia/\"\n",
        "PATH_DONNEES_BRUTES = f\"{my_local_drive}donnees_brutes/{CURR_FOLDER}\"\n",
        "PATH_OUTPUT = f\"{my_local_drive}output/{CURR_FOLDER}\"\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional\n",
        "import spacy\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "from numpy.linalg import norm\n",
        "from collections import defaultdict\n",
        "import glob\n",
        "import csv\n",
        "import requests, json, time\n",
        "from tqdm import tqdm\n",
        "import string\n",
        "\n",
        "nlp = spacy.load(\"fr_core_news_lg\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbyIjHUzayfw",
        "outputId": "7b95f1c6-3280-41e0-85a7-87669f1ed6ca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class TokenAnnotation:\n",
        "    text: str           # surface form\n",
        "    lemma: str          # canonical form\n",
        "    pos: str            # part-of-speech tag\n",
        "    dep: str            # dependency label\n",
        "    head: int           # index of the head token in its sentence\n",
        "    sent_id: Optional[int] = None  # optional: which sentence\n",
        "\n",
        "def annotate(text: str) -> List[List[TokenAnnotation]]:\n",
        "    doc = nlp(text)\n",
        "    all_sents: List[List[TokenAnnotation]] = []\n",
        "    for sent_id, sent in enumerate(doc.sents):\n",
        "        tokens = []\n",
        "        for token in sent:\n",
        "            tokens.append(TokenAnnotation(\n",
        "                text=token.text,\n",
        "                lemma=token.lemma_,\n",
        "                pos=token.pos_,\n",
        "                dep=token.dep_,\n",
        "                head=token.head.i - sent.start,  # index *within* this sentence\n",
        "                sent_id=sent_id\n",
        "            ))\n",
        "        all_sents.append(tokens)\n",
        "    return all_sents\n",
        "\n",
        "INTERESTING_DEPS = {\n",
        "    # verbal relations: subject → verb, verb → object\n",
        "    \"nsubj\", \"nsubj:pass\", \"obj\", \"iobj\",\n",
        "    # noun modifiers: noun → noun (prep-headed), noun → adjective\n",
        "    \"nmod\", \"obl\",  \"amod\",    # e.g. “recette de cuisine”, “bouillon aromatique”\n",
        "    # multi-word names & appositions\n",
        "    \"compound\", \"flat:name\", \"appos\",\n",
        "    # clausal modifiers (relative/participial clauses)\n",
        "    \"acl\", \"acl:relcl\", \"advcl\",\n",
        "}\n",
        "\n",
        "def extract_pairs(tokens):\n",
        "    pairs = []\n",
        "    for idx, tok in enumerate(tokens):\n",
        "        if tok.dep in INTERESTING_DEPS:\n",
        "            head = tokens[tok.head]\n",
        "            if tok.lemma in string.punctuation or head.lemma in string.punctuation:\n",
        "                continue\n",
        "            pairs.append(( head.lemma, tok.lemma, tok.dep, tok.pos ))\n",
        "    return pairs\n",
        "\n",
        "model = KeyedVectors.load_word2vec_format('cc.fr.300.vec.gz', binary=False, limit=50000)  # Limit to 50k for memory\n",
        "\n",
        "# 1. Define your gastronomy “pivot” words\n",
        "pivots = [\"cuisine\",\"recette\",\"ingrédient\",\"épice\",\"gastronomie\"]\n",
        "# Keep only those that actually exist in the model\n",
        "pivot_vecs = [model[p] for p in pivots if p in model]\n",
        "if not pivot_vecs:\n",
        "    raise ValueError(\"None of your pivots were in the model!\")\n",
        "\n",
        "# 2. Compute the domain centroid\n",
        "domain_centroid = np.mean(pivot_vecs, axis=0)\n",
        "\n",
        "# 4. Define helper functions\n",
        "def pair_vector(h, lemma):\n",
        "    \"\"\"Average the two word vectors.\"\"\"\n",
        "    return (model[h] + model[lemma]) / 2\n",
        "\n",
        "def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    return float(a.dot(b) / (norm(a) * norm(b)))"
      ],
      "metadata": {
        "id": "kSXVi4hT8sUh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "BASE_URL = \"https://jdm-api.demo.lirmm.fr/v0/relations\"\n",
        "# Assuming you want to process all .txt files in the PATH_DONNEES_BRUTES folder\n",
        "THRESHOLD = 0.25\n",
        "\n",
        "# Configure logging\n",
        "log_file_path = os.path.join(PATH_OUTPUT, 'jdm_errors.log')\n",
        "logging.basicConfig(filename=log_file_path, level=logging.ERROR,\n",
        "                    format='%(asctime)s - %(levelname)s - %(filename)s - %(message)s')\n",
        "\n",
        "processed_files = [os.path.basename(f).replace(\"_relations.csv\", \".txt\")\n",
        "                   for f in glob.glob(os.path.join(PATH_OUTPUT + \"relations\", \"*_relations.csv\"))]\n",
        "print(processed_files)\n",
        "\n",
        "for filename in glob.glob(os.path.join(PATH_DONNEES_BRUTES, \"*.txt\")):\n",
        "    # Extract filename without extension for comparison\n",
        "    base_filename = os.path.basename(filename)\n",
        "    if base_filename in processed_files:\n",
        "        print(f\"Skipping already processed file: {filename}\")\n",
        "        continue  # Skip to the next file\n",
        "\n",
        "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
        "        content = f.read()\n",
        "\n",
        "    # Process the content of each file as before\n",
        "    sents = annotate(content)\n",
        "    all_pairs = []\n",
        "    for sent in sents:\n",
        "        p = extract_pairs(sent)\n",
        "        all_pairs.append(p)\n",
        "\n",
        "    filtered = []\n",
        "    for i, pair in enumerate(all_pairs):\n",
        "        for h, lemma, dep, pos in pair:\n",
        "            if h in model and lemma in model:\n",
        "                vec = pair_vector(h, lemma)\n",
        "                sim = cosine_sim(domain_centroid, vec)\n",
        "                # if sim >= THRESHOLD:\n",
        "                filtered.append((h, lemma, dep, pos, sim))\n",
        "\n",
        "    seen_pairs = set()  # Keep track of pairs we've already written\n",
        "    # Create the directory if it doesn't exist\n",
        "    pairs_output_dir = os.path.join(PATH_OUTPUT, 'pairs')\n",
        "    relations_output_dir = os.path.join(PATH_OUTPUT, 'relations')\n",
        "    os.makedirs(pairs_output_dir, exist_ok=True)\n",
        "    os.makedirs(relations_output_dir, exist_ok=True)\n",
        "\n",
        "    output_filename_pairs = os.path.join(pairs_output_dir, os.path.basename(filename).replace(\".txt\", \"_pairs.txt\"))\n",
        "\n",
        "    with open(output_filename_pairs, \"w\") as f:\n",
        "        for h, lemma, dep, pos, sim in filtered:\n",
        "            pair = (h, lemma)\n",
        "            if pair not in seen_pairs:\n",
        "                f.write(f\"{h},{lemma},{dep},{pos},{sim}\\n\")\n",
        "                seen_pairs.add(pair)\n",
        "\n",
        "    # Example:  Adapt the JDM relation extraction\n",
        "    output_filename_relations = os.path.join(relations_output_dir, os.path.basename(filename).replace(\".txt\", \"_relations.csv\"))\n",
        "\n",
        "    with open(output_filename_pairs, encoding=\"utf-8\") as fin, \\\n",
        "        open(output_filename_relations, \"w\", newline=\"\", encoding=\"utf-8\") as fout:\n",
        "\n",
        "        writer = csv.DictWriter(fout, fieldnames=[\"node1\",\"node2\",\"relations\"])\n",
        "        writer.writeheader()\n",
        "\n",
        "        # Wrap the loop with tqdm to create a progress bar\n",
        "        for line in tqdm(fin, desc=f\"Processing pairs from {os.path.basename(filename)}\"): # use file name in the description\n",
        "            try:\n",
        "                head, lemma, dep, pos, sim = line.strip().split(\",\")\n",
        "                sim = float(sim)\n",
        "                if sim >= THRESHOLD:\n",
        "                    resp = requests.get(f\"{BASE_URL}/from/{head}/to/{lemma}\")\n",
        "                    resp.raise_for_status()\n",
        "                    rels = resp.json().get(\"relations\", [])\n",
        "                    writer.writerow({\n",
        "                        \"node1\": head,\n",
        "                        \"node2\": lemma,\n",
        "                        \"relations\": json.dumps(rels, ensure_ascii=False),\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                logging.error(f\"File: {os.path.basename(filename)}, Pair: ({head}, {lemma}), Error: {e}\")\n",
        "                print(f\"Error processing pair ({head}, {lemma}): {e}\")\n",
        "                time.sleep(0.2)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W5gyrPlY0uDN",
        "outputId": "17b1efae-9992-4cb0-e3d9-bb9c18c339db"
      },
      "execution_count": 4,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Kamo.txt', 'Baccalà alla lucana.txt', 'Bäck _gastronomie_.txt', 'Batida.txt', 'Houttuynia cordata.txt', 'Labatt Bleue.txt', 'Makroud.txt', 'Tokkuri.txt', 'Leipäjuusto.txt', 'Sandwich polonais de Chicago.txt', 'Cugnot _pâtisserie_.txt', 'Ghiveci.txt', 'Pâté chinois.txt', 'Mghellef fi ghlâfou.txt', 'Brisas do Lis.txt', 'Spettekaka.txt', 'Boule _pain_.txt', 'Kepiting saus Padang.txt', 'Biscuits feuille d_érable.txt', 'Mole rosa.txt', 'Oroshigane.txt', 'Noma _restaurant_.txt', 'Tekkadon.txt', 'Sauce chien.txt', 'Mojama.txt', 'Barquillo.txt', 'Sauce américaine.txt', 'Tortillon.txt', 'Tilslørte bondepiker.txt', 'Soudjouk.txt', 'Poulet à la Kiev.txt', 'Sferies.txt', 'Fattouche.txt', 'Manjū.txt', 'Botamochi.txt', 'Balushahi.txt', 'Otak-otak.txt', 'Palette Fromages danois.txt', 'Croquant de Cordes.txt', 'Fromage en grains.txt', 'Grelos.txt', 'Palačinka.txt', 'Poisson dans la cuisine japonaise.txt', 'Kaldi.txt', 'Toffee éponge.txt', 'Focaccia novese.txt', 'Warung.txt', 'Brandy snap.txt', 'Chahda.txt', 'Tarallo.txt']\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Kepiting saus Padang.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Biscuits feuille d_érable.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Mole rosa.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Kamo.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Baccalà alla lucana.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Bäck _gastronomie_.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Batida.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Houttuynia cordata.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Labatt Bleue.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Makroud.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Tokkuri.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Leipäjuusto.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Sandwich polonais de Chicago.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Cugnot _pâtisserie_.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Ghiveci.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Pâté chinois.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Mghellef fi ghlâfou.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Brisas do Lis.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Spettekaka.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Boule _pain_.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Oroshigane.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Noma _restaurant_.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Tekkadon.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Sauce chien.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Mojama.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Barquillo.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Sauce américaine.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Tortillon.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Tilslørte bondepiker.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Soudjouk.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Poulet à la Kiev.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Sferies.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Fattouche.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Manjū.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Botamochi.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Balushahi.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Otak-otak.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Palette Fromages danois.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Croquant de Cordes.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Fromage en grains.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Grelos.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Palačinka.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Poisson dans la cuisine japonaise.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Kaldi.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Toffee éponge.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Focaccia novese.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Warung.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Brandy snap.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Chahda.txt\n",
            "Skipping already processed file: /content/gdrive/My Drive/TER/donnees_brutes/wikipedia/Tarallo.txt\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing pairs from Poulpe à la provençale.txt: 47it [00:27,  1.71it/s]\n",
            "Processing pairs from Shōgayaki.txt: 19it [00:12,  1.58it/s]\n",
            "Processing pairs from Apfelsaftschorle.txt: 53it [00:29,  1.83it/s]\n",
            "Processing pairs from Saucisse de couenne.txt: 60it [00:29,  2.06it/s]\n",
            "Processing pairs from Boumehras.txt: 35it [00:18,  1.91it/s]\n",
            "Processing pairs from Kompot.txt: 104it [00:58,  1.76it/s]\n",
            "Processing pairs from Légume japonais.txt: 2it [00:01,  1.48it/s]\n",
            "Processing pairs from Katsudon.txt: 24it [00:13,  1.77it/s]\n",
            "Processing pairs from Devil_s food cake.txt: 49it [00:28,  1.75it/s]\n",
            "Processing pairs from Kahwah.txt: 31it [00:16,  1.86it/s]\n",
            "Processing pairs from Hwangnam-ppang.txt: 15it [00:08,  1.85it/s]\n",
            "Processing pairs from Eliza Acton.txt: 334it [02:06,  1.71it/s]ERROR:root:File: Eliza Acton.txt, Pair: (œuvre, Modern), Error: 500 Server Error: Internal Server Error for url: https://jdm-api.demo.lirmm.fr/v0/relations/from/%C5%93uvre/to/Modern\n",
            "Processing pairs from Eliza Acton.txt: 335it [02:07,  1.50it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error processing pair (œuvre, Modern): 500 Server Error: Internal Server Error for url: https://jdm-api.demo.lirmm.fr/v0/relations/from/%C5%93uvre/to/Modern\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:root:File: Eliza Acton.txt, Pair: (recevoir, œuvre), Error: 500 Server Error: Internal Server Error for url: https://jdm-api.demo.lirmm.fr/v0/relations/from/recevoir/to/%C5%93uvre\n",
            "Processing pairs from Eliza Acton.txt: 336it [02:08,  1.39it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error processing pair (recevoir, œuvre): 500 Server Error: Internal Server Error for url: https://jdm-api.demo.lirmm.fr/v0/relations/from/recevoir/to/%C5%93uvre\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:root:File: Eliza Acton.txt, Pair: (œuvre, art), Error: 500 Server Error: Internal Server Error for url: https://jdm-api.demo.lirmm.fr/v0/relations/from/%C5%93uvre/to/art\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error processing pair (œuvre, art): 500 Server Error: Internal Server Error for url: https://jdm-api.demo.lirmm.fr/v0/relations/from/%C5%93uvre/to/art\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing pairs from Eliza Acton.txt: 337it [02:09,  1.31it/s]ERROR:root:File: Eliza Acton.txt, Pair: (œuvre, nourriture), Error: 500 Server Error: Internal Server Error for url: https://jdm-api.demo.lirmm.fr/v0/relations/from/%C5%93uvre/to/nourriture\n",
            "Processing pairs from Eliza Acton.txt: 338it [02:10,  1.25it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error processing pair (œuvre, nourriture): 500 Server Error: Internal Server Error for url: https://jdm-api.demo.lirmm.fr/v0/relations/from/%C5%93uvre/to/nourriture\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing pairs from Eliza Acton.txt: 494it [03:20,  1.86it/s]ERROR:root:File: Eliza Acton.txt, Pair: (accomplir, œuvre), Error: 500 Server Error: Internal Server Error for url: https://jdm-api.demo.lirmm.fr/v0/relations/from/accomplir/to/%C5%93uvre\n",
            "Processing pairs from Eliza Acton.txt: 496it [03:20,  1.98it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error processing pair (accomplir, œuvre): 500 Server Error: Internal Server Error for url: https://jdm-api.demo.lirmm.fr/v0/relations/from/accomplir/to/%C5%93uvre\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing pairs from Eliza Acton.txt: 835it [06:11,  2.25it/s]\n",
            "Processing pairs from Paskha.txt: 84it [00:43,  1.92it/s]\n",
            "Processing pairs from Salade niçoise.txt: 145it [01:45,  1.37it/s]\n",
            "Processing pairs from Massepain cuit.txt: 28it [00:18,  1.55it/s]\n",
            "Processing pairs from Halocynthia roretzi.txt: 84it [00:49,  1.71it/s]\n",
            "Processing pairs from Doigts de Fatma.txt: 41it [00:25,  1.62it/s]\n",
            "Processing pairs from Champurrado.txt: 20it [00:12,  1.64it/s]\n",
            "Processing pairs from Cassata.txt: 82it [00:54,  1.50it/s]\n",
            "Processing pairs from Panforte.txt: 214it [02:08,  1.67it/s]\n",
            "Processing pairs from Sandwich italien.txt: 3it [00:02,  1.49it/s]\n",
            "Processing pairs from Fassbrause.txt: 237it [01:53,  2.08it/s]\n",
            "Processing pairs from Grotte à la framboise.txt: 19it [00:10,  1.86it/s]\n",
            "Processing pairs from Tarte à la crème de Boston.txt: 100it [01:03,  1.57it/s]\n",
            "Processing pairs from Rendang.txt: 34it [00:25,  1.35it/s]\n",
            "Processing pairs from Key lime pie.txt: 56it [00:30,  1.82it/s]\n",
            "Processing pairs from Menma.txt: 14it [00:10,  1.33it/s]\n",
            "Processing pairs from Sélection Caseus.txt: 19it [00:09,  1.99it/s]\n",
            "Processing pairs from Crêpe.txt: 396it [03:55,  1.68it/s]\n",
            "Processing pairs from Choujiu.txt: 56it [00:25,  2.22it/s]\n",
            "Processing pairs from Thé de chrysanthème.txt: 153it [01:24,  1.80it/s]\n",
            "Processing pairs from Kägi fret.txt: 183it [01:35,  1.91it/s]\n",
            "Processing pairs from Kuzuko.txt: 30it [00:19,  1.56it/s]\n",
            "Processing pairs from Ouazouaza.txt: 22it [00:10,  2.01it/s]\n",
            "Processing pairs from Cuscuz.txt: 41it [00:26,  2.82it/s]ERROR:root:File: Cuscuz.txt, Pair: (colonie, portugais), Error: 504 Server Error: Gateway Time-out for url: https://jdm-api.demo.lirmm.fr/v0/relations/from/colonie/to/portugais\n",
            "Processing pairs from Cuscuz.txt: 42it [01:27, 10.74s/it]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error processing pair (colonie, portugais): 504 Server Error: Gateway Time-out for url: https://jdm-api.demo.lirmm.fr/v0/relations/from/colonie/to/portugais\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing pairs from Cuscuz.txt: 49it [01:34,  1.94s/it]\n",
            "Processing pairs from Bakbouka.txt: 25it [00:15,  1.57it/s]\n",
            "Processing pairs from Licitar.txt: 126it [01:19,  1.59it/s]\n",
            "Processing pairs from Taj el moulouk.txt: 19it [00:11,  1.59it/s]\n",
            "Processing pairs from Sauce béchamel.txt: 91it [00:51,  1.75it/s]\n",
            "Processing pairs from Soupe de poireaux.txt: 48it [00:30,  1.59it/s]\n",
            "Processing pairs from Pemmican.txt: 155it [01:21,  1.90it/s]\n",
            "Processing pairs from Mona de Pascua.txt: 2it [00:00,  2.99it/s]\n",
            "Processing pairs from Zirawi.txt: 24it [00:15,  1.59it/s]\n",
            "Processing pairs from Pinyata de Collioure.txt: 16it [00:11,  1.42it/s]\n",
            "Processing pairs from Nian gao.txt: 43it [00:26,  1.62it/s]\n",
            "Processing pairs from Raclette.txt: 2it [00:01,  1.43it/s]\n",
            "Processing pairs from Mirin.txt: 150it [01:39,  1.51it/s]\n",
            "Processing pairs from Pompe aux pommes.txt: 31it [00:17,  1.75it/s]\n",
            "Processing pairs from Trempette.txt: 11it [00:10,  1.05it/s]\n",
            "Processing pairs from Brissauda.txt: 89it [00:55,  1.60it/s]\n",
            "Processing pairs from Gaspacho manchois.txt: 142it [01:21,  1.73it/s]\n",
            "Processing pairs from Frite de patate douce.txt: 7it [00:04,  1.70it/s]\n",
            "Processing pairs from Tarte au sucre.txt: 36it [00:25,  1.42it/s]\n",
            "Processing pairs from Makdous.txt: 31it [00:16,  1.83it/s]\n",
            "Processing pairs from Tumbet.txt: 40it [00:21,  1.89it/s]\n",
            "Processing pairs from Sbitène.txt: 20it [00:14,  1.41it/s]\n",
            "Processing pairs from Osechi.txt: 82it [00:47,  1.72it/s]\n",
            "Processing pairs from Gruau de fruit.txt: 78it [00:46,  1.67it/s]\n",
            "Processing pairs from Sarma.txt: 52it [00:28,  1.85it/s]\n",
            "Processing pairs from Tarte aux quetsches.txt: 6it [00:03,  1.76it/s]\n",
            "Processing pairs from Biétrumé.txt: 25it [00:13,  1.92it/s]\n",
            "Processing pairs from Tarte à masteilles.txt: 47it [00:23,  1.97it/s]\n",
            "Processing pairs from Cuisine poitevine.txt: 39it [00:26,  1.48it/s]\n",
            "Processing pairs from Irn-Bru.txt: 228it [01:47,  2.12it/s]\n",
            "Processing pairs from Kaya.txt: 65it [00:41,  1.56it/s]\n",
            "Processing pairs from Tankboy.txt: 35it [00:19,  1.83it/s]\n",
            "Processing pairs from Miyeok-guk.txt: 67it [00:37,  1.80it/s]\n",
            "Processing pairs from Slilet.txt: 21it [00:15,  1.38it/s]\n",
            "Processing pairs from Sauce aux canneberges.txt: 61it [00:36,  1.67it/s]\n",
            "Processing pairs from Bourride à la sétoise.txt: 74it [00:48,  1.53it/s]\n",
            "Processing pairs from Pox.txt: 67it [00:36,  1.83it/s]\n",
            "Processing pairs from Borona.txt: 108it [00:58,  1.84it/s]\n",
            "Processing pairs from Glace à la pâte à cookies aux pépites de chocolat.txt: 62it [00:37,  1.66it/s]\n",
            "Processing pairs from Pambazo.txt: 52it [00:26,  1.98it/s]\n",
            "Processing pairs from Beguni.txt: 46it [00:25,  1.81it/s]\n",
            "Processing pairs from Cougnou.txt: 287it [02:25,  1.97it/s]\n",
            "Processing pairs from Kattama.txt: 10it [00:06,  1.48it/s]\n",
            "Processing pairs from Fromage anglais.txt: 3it [00:02,  1.45it/s]\n",
            "Processing pairs from Saucisse de Molène.txt: 11it [00:06,  1.80it/s]\n",
            "Processing pairs from Ensaïmada.txt: 116it [01:02,  1.85it/s]\n",
            "Processing pairs from tikka masala.txt: 3it [00:02,  1.26it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-040d67f1e6d1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msim\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mTHRESHOLD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{BASE_URL}/from/{head}/to/{lemma}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m                     \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                     \u001b[0mrels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"relations\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0;31m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0mserver_hostname_rm_dot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m             sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n\u001b[0m\u001b[1;32m    742\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                 \u001b[0mcert_reqs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcert_reqs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[0;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[1;32m    918\u001b[0m             \u001b[0mserver_hostname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m     ssl_sock = ssl_wrap_socket(\n\u001b[0m\u001b[1;32m    921\u001b[0m         \u001b[0msock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0mkeyfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpn_protocols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mALPN_PROTOCOLS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m     \u001b[0mssl_sock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ssl_wrap_socket_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtls_in_tls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mssl_sock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mSSLTransport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssl_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mssl_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;31m# SSLSocket class handles server_hostname encoding before it calls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0;31m# ctx._wrap_socket()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         return self.sslsocket_class._create(\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0msock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0mserver_side\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_side\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36m_create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1102\u001b[0m                         \u001b[0;31m# non-blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Approche initiale (Ça sert à rien pour vous)"
      ],
      "metadata": {
        "id": "1lAEKNygpVEZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dYDEjPsxfVtF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b830d8c-aecf-4c3d-da73-dd81832071d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "my_local_drive='/content/gdrive/My Drive/TER/'\n",
        "sys.path.append(my_local_drive)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_RELATIONS_TYPES = f\"{my_local_drive}relations_types.json\" # Liste des relations\n",
        "PATH_PAIRS = f\"{my_local_drive}pairs.txt\" # Paires récupérées\n",
        "PATH_FILTERED_PAIRS = f\"{my_local_drive}filtered_pairs.txt\" # Paires filtrées par Word2Vec\n",
        "PATH_UNDER_PAIRS = f\"{my_local_drive}under_threshold_pairs.txt\"\n",
        "PATH_RELATIONS_RESULTS = f\"{my_local_drive}relations_results.csv\" # Résultats retournés de JdM\n",
        "\n",
        "CURR_FOLDER = \"wikipedia/\"\n",
        "PATH_DONNEES_BRUTES = f\"{my_local_drive}donnees_brutes/{CURR_FOLDER}\"\n",
        "PATH_OUTPUT = f\"{my_local_drive}output/{CURR_FOLDER}\""
      ],
      "metadata": {
        "id": "iq98sQiLO3mp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install wikipedia-api"
      ],
      "metadata": {
        "id": "Vo-EAPmjzwh0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import wikipediaapi\n",
        "# import re\n",
        "# from urllib.parse import unquote\n",
        "\n",
        "# # url = \"https://fr.wikipedia.org/wiki/Pot-au-feu\"  # Change this to any Wikipedia URL\n",
        "# url = \"https://fr.wikipedia.org/wiki/Tripes_%C3%A0_la_proven%C3%A7ale\"\n",
        "\n",
        "# def get_wikipedia_text(url, lang='fr'):\n",
        "#     # Extract the title from the Wikipedia URL\n",
        "#     decoded_url = unquote(url)\n",
        "#     match = re.search(r\"/wiki/(.+)$\", decoded_url)\n",
        "#     if not match:\n",
        "#         print(\"Invalid Wikipedia URL.\")\n",
        "#         return None\n",
        "\n",
        "#     page_title = match.group(1).replace('_', ' ')\n",
        "\n",
        "#     wiki_wiki = wikipediaapi.Wikipedia(language=lang, user_agent=\"MyWikipediaScraper/1.0\")\n",
        "#     page = wiki_wiki.page(page_title)\n",
        "\n",
        "#     if not page.exists():\n",
        "#         print(f\"Page '{page_title}' does not exist.\")\n",
        "#         return None\n",
        "\n",
        "#     return page.text\n",
        "\n",
        "# text = get_wikipedia_text(url)\n",
        "\n",
        "# # Save to a text file\n",
        "# if text:\n",
        "#     with open(f\"{my_local_drive}wikipedia_text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "#         f.write(text)\n",
        "#     print(f\"Text saved to wikipedia_text.txt\")\n"
      ],
      "metadata": {
        "id": "xLsPnejMhTOi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# content = \"\"\n",
        "# with open(f\"{my_local_drive}wikipedia_text.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "#   content = f.read();"
      ],
      "metadata": {
        "id": "tLUPnr3QjxIt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download fr_core_news_lg"
      ],
      "metadata": {
        "id": "3i9QYtQtqE70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import List, Optional\n",
        "import spacy\n",
        "nlp = spacy.load(\"fr_core_news_lg\")\n",
        "\n",
        "@dataclass\n",
        "class TokenAnnotation:\n",
        "    text: str           # surface form\n",
        "    lemma: str          # canonical form\n",
        "    pos: str            # part-of-speech tag\n",
        "    dep: str            # dependency label\n",
        "    head: int           # index of the head token in its sentence\n",
        "    sent_id: Optional[int] = None  # optional: which sentence\n",
        "\n",
        "def annotate(text: str) -> List[List[TokenAnnotation]]:\n",
        "    doc = nlp(text)\n",
        "    all_sents: List[List[TokenAnnotation]] = []\n",
        "    for sent_id, sent in enumerate(doc.sents):\n",
        "        tokens = []\n",
        "        for token in sent:\n",
        "            tokens.append(TokenAnnotation(\n",
        "                text=token.text,\n",
        "                lemma=token.lemma_,\n",
        "                pos=token.pos_,\n",
        "                dep=token.dep_,\n",
        "                head=token.head.i - sent.start,  # index *within* this sentence\n",
        "                sent_id=sent_id\n",
        "            ))\n",
        "        all_sents.append(tokens)\n",
        "    return all_sents\n",
        "\n",
        "# Exécution\n",
        "# Utiliser ça au lieu du texte brut de Wikipedia pour tester\n",
        "# test_text = \"\"\"Le pot-au-feu (inv.) est une recette de cuisine traditionnelle emblématique historique de la cuisine française, et du repas gastronomique des Français, à base de viande de bœuf cuisant longuement à feu très doux dans un bouillon de légumes (poireau, carotte, navet, oignon, céleri, chou et bouquet garni). La présence de pommes de terre est discutée, puisqu’elles ne faisaient pas partie de la recette d’origine, la pomme de terre n’ayant été introduite en France par Antoine Parmentier qu’à la fin du XVIIIe siècle. Historiquement, c’est plutôt le panais qui jouait son rôle.\n",
        "# Historique\n",
        "# Jean Louis Schefer fait remonter le pot-au-feu au rêve néolithique, « celui du foyer, du vase d'argile, du pot mis au feu, de la soif étanchée, de la faim apaisée… », origine reprise par le restaurant À la Cloche d'or : « Pot-au-feu désigne à la base le « pot à feu », le pot dans lequel on faisait revenir un bouillon aromatique auquel on ajoutait viandes et légumes ». Jean Guillaume pense qu'à l'origine de l'agriculture les raves sont venues compléter les herbes dans les bouillons, « on y ajoutait du pain pour faire la soupe et de la viande pour les grands jours ».\n",
        "# Au XIIIe siècle, il est appelé « viande au pot ». Autrefois, la cuisson du pot-au-feu pouvait s’effectuer de façon continue, de nouveaux ingrédients étant rajoutés au fur et à mesure pour remplacer ceux qui étaient retirés afin d’être consommés. À présent que les maisons n’ont plus un feu de bois allumé en continu, le pot-au-feu est cuisiné spécifiquement en vue d’un repas.\n",
        "# Marcel Rouff, dans son roman Vie et Passion de Dodin-Bouffant, gourmet (1924) a décrit un pot-au-feu devenu mythique, qui a inspiré des pots-au-feu démesurés à de nombreux chefs.\n",
        "# Composition\n",
        "# Les coupes de bœuf et les légumes impliqués varient, mais un pot-au-feu typique contient :\n",
        "# des coupes de bœuf à faible coût nécessitant une longue cuisson : gîte, gîte à la noix, joue de bœuf, jarret, plat de côtes, paleron, macreuse à pot-au-feu ou jumeau à pot-au-feu ;\n",
        "# classiquement fait avec du bœuf ou du poulet, parfois veau, porc ou mouton sont utilisés ;\n",
        "# un ou plusieurs morceaux cartilagineux : queue de bœuf ou os à moelle ;\n",
        "# des légumes : carotte, navet, poireau, parfois pomme de terre (qui n’a été introduite que tard, au cours du XVIIIe siècle au moment de sa promotion en France par Antoine Parmentier), céleri-rave, oignon (selon les régions et les recettes) ;\n",
        "# des épices : bouquet garni, sel, poivre noir et clous de girofle.\n",
        "# Le pot-au-feu est l'un des rares plats où l'on utilise parfois des aliments brûlés : pour parfumer et colorer le bouillon, les oignons sont coupés en deux et passés au four (gril) jusqu'à ce que la surface soit complètement noire.\n",
        "# Jules Gouffé, cuisinier et pâtissier français du XIXe siècle distingue le petit pot-au-feu ordinaire du grand pot-au-feu des jours d'extra.\n",
        "# Cuisson\n",
        "# Deux méthodes sont en présence : mettre le bœuf dans l'eau froide ou bien dans l'eau bouillante. La première donne un bouillon succulent, la seconde préserve davantage le gout des viandes. Paul Bocuse, Jules Gouffé optent  pour l'eau froide. Un bon compromis selon Sabine Jeannin et al. est de commencer à l'eau froide avec un premier morceau de bœuf, et d'en ajouter un autre quand l'eau bout.\n",
        "# Le bouillon contient traditionnellement un oignon piqué de clous de girofle, l'ail est déconseillé. La cuisson doit être longue et douce, (« le premier soin est de bien faire son feu »), ébullition continue et régulière pendant 3 à 5 heures selon le contenu, trop cuire le pot-au-feu est néfaste — les légumes ne séjournent dans le bouillon que le temps de les cuire —, on laisse entrouvert le couvercle de la marmite. Le bouillon est écumé en début de cuisson puis dégraissé après avoir retiré la viande cuite. Les amateurs préfèrent le pot-au-feu réchauffé le lendemain.\n",
        "# Dans les autocuiseurs, la cuisson se fait toujours en deux temps, le second est bref et réservé aux légumes.\n",
        "# Service\n",
        "# Le bouillon de cuisson du pot-au-feu est servi à côté comme potage, souvent agrémenté de pâtes, riz ou pain grillé, au dîner ou en entrée avant de servir la viande et les légumes du pot-au-feu. Il sert également de base aux sauces ou à la cuisson des légumes ou des pâtes. La moelle est mangée sur du pain grillé. Ensuite, le pot-au-feu est généralement servi avec du gros sel et de la moutarde forte de Dijon. Le reste de viande peut être broyé et utilisé pour la préparation d'un pâté de viande, mais cette pratique est rare en France, sauf en Alsace où la viande et le bouillon servent à cuisiner les Fleischschnacka.\n",
        "# Accord mets/vin\n",
        "# Le vin blanc est rarement proposé avec le pot-au-feu. Il s'accorde pourtant avec ce mets s'il est ample et vif, dans ce cas, c'est un vin qui désaltère et met en appétit.\n",
        "# Le vin rosé à conseiller doit être sec, corsé, avec une robe rose-rouge qui témoigne de sa charge en matières sèches. Ce type de vin s'accorde avec les légumes et étanche la soif.\n",
        "# Le vin rouge offre une large gamme qui va des bourgognes aux bordeaux, en passant par les beaujolais, les côtes-du-rhône-villages, les coteaux-du-languedoc,.\"\"\"\n",
        "\n",
        "# sents = annotate(test_text)\n",
        "# for sent in sents:\n",
        "#     print(sent)\n"
      ],
      "metadata": {
        "id": "QIMlLE2ItHUn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Approche initiale"
      ],
      "metadata": {
        "id": "eQw_0tpYQW8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from spacy import displacy\n",
        "# tesssst = nlp(\"Le bouillon contient traditionnellement un oignon piqué de clous de girofle, l'ail est déconseillé.\")\n",
        "# displacy.serve(tesssst, style=\"dep\")"
      ],
      "metadata": {
        "id": "uAjlOMhy1Xl0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# POUR VOIR LES EXPLICATIONS DES LABELS `dep`\n",
        "# for label in nlp.get_pipe(\"parser\").labels:\n",
        "#     print(label, \" -- \", spacy.explain(label))\n"
      ],
      "metadata": {
        "id": "5GsJAI7v18CV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explication des DEPS intéressés\n",
        "\n",
        "  1.\tDepLabel : nsubj\n",
        "\t•\tPourquoi : lie un verbe à son sujet (agent)\n",
        "\t•\tExemple & paire extraite : « Le pot-au-feu est une recette. » → (recette, pot-au-feu)\n",
        "\n",
        "  2.\tDepLabel : nsubj:pass\n",
        "\t•\tPourquoi : lie un verbe passif à son sujet/patient\n",
        "\t•\tExemple & paire extraite : « La présence de pomme de terre est discutée. » → (discutée, présence)\n",
        "\n",
        "  3.\tDepLabel : obj\n",
        "\t•\tPourquoi : lie un verbe à son objet direct (quoi fait-on ?)\n",
        "\t•\tExemple & paire extraite : « Schefer fait remonter le pot-au-feu. » → (remonter, pot-au-feu)\n",
        "\n",
        "  4.\tDepLabel : iobj\n",
        "\t•\tPourquoi : lie un verbe à son objet indirect (qui reçoit l’action)\n",
        "\t•\tExemple & paire extraite : « un bouillon auquel on ajoutait viandes. » → (ajoutait, auquel)\n",
        "\n",
        "  5.\tDepLabel : nmod\n",
        "\t•\tPourquoi : complément nominal via préposition (“de”, “à”, …)\n",
        "\t•\tExemple & paire extraite : « recette de cuisine » → (recette, cuisine)\n",
        "\n",
        "  6.\tDepLabel : obl\n",
        "\t•\tPourquoi : complément oblique (lieu, instrument, but…)\n",
        "\t•\tExemple & paire extraite : « cuisant à feu doux » → (cuisant, feu)\n",
        "\n",
        "  7.\tDepLabel : amod\n",
        "\t•\tPourquoi : lie un nom à son adjectif qualificatif\n",
        "\t•\tExemple & paire extraite : « recette traditionnelle » → (recette, traditionnelle)\n",
        "\n",
        "  8.\tDepLabel : compound\n",
        "\t•\tPourquoi : reconstitue des termes composés (expressions figées)\n",
        "\t•\tExemple & paire extraite : « pot-au-feu » → (pot-au-feu, feu)\n",
        "\n",
        "  9.\tDepLabel : flat:name\n",
        "\t•\tPourquoi : agrège des noms propres formant une seule entité\n",
        "\t•\tExemple & paire extraite : « Jean Louis Schefer » → (Jean, Louis)\n",
        "\n",
        "  10.\tDepLabel : appos\n",
        "\t•\tPourquoi : apposition — le second nom renomme le premier\n",
        "\t•\tExemple & paire extraite : « bouillon, poireau, carotte… » → (bouillon, poireau)\n",
        "\n",
        "  11.\tDepLabel : acl\n",
        "\t•\tPourquoi : clause participiale attachée à un nom\n",
        "\t•\tExemple & paire extraite : « pot mis au feu » → (pot, mis)\n",
        "\n",
        "  12.\tDepLabel : acl:relcl\n",
        "\t•\tPourquoi : proposition relative attachée à un nom\n",
        "\t•\tExemple & paire extraite : « le panais qui jouait son rôle » → (panais, jouait)\n",
        "\n",
        "  13.\tDepLabel : advcl\n",
        "\t•\tPourquoi : clause adverbiale attachée à un verbe\n",
        "\t•\tExemple & paire extraite : « le bouillon contient … l’ail est déconseillé » → (contient, déconseillé)"
      ],
      "metadata": {
        "id": "IzNkaOEb93eD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INTERESTING_DEPS = {\n",
        "    # verbal relations: subject → verb, verb → object\n",
        "    \"nsubj\", \"nsubj:pass\", \"obj\", \"iobj\",\n",
        "    # noun modifiers: noun → noun (prep-headed), noun → adjective\n",
        "    \"nmod\", \"obl\",  \"amod\",    # e.g. “recette de cuisine”, “bouillon aromatique”\n",
        "    # multi-word names & appositions\n",
        "    \"compound\", \"flat:name\", \"appos\",\n",
        "    # clausal modifiers (relative/participial clauses)\n",
        "    \"acl\", \"acl:relcl\", \"advcl\",\n",
        "}\n",
        "\n",
        "def extract_pairs(tokens):\n",
        "    pairs = []\n",
        "    for idx, tok in enumerate(tokens):\n",
        "        if tok.dep in INTERESTING_DEPS:\n",
        "            head = tokens[tok.head]\n",
        "            pairs.append(( head.lemma, tok.lemma, tok.dep, tok.pos ))\n",
        "    return pairs\n",
        "\n",
        "# all_pairs = []\n",
        "# for sent in sents:\n",
        "#     p = extract_pairs(sent)\n",
        "#     all_pairs.append(p)"
      ],
      "metadata": {
        "id": "aQgKuDRYCXzB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"HEAD | LEMMA | DEP | POS\")\n",
        "for i, pair in enumerate(all_pairs[:1]):\n",
        "    for p in pair: print(p)\n",
        "\n",
        "with open(PATH_PAIRS, \"w\") as f:\n",
        "    for i, pair in enumerate(all_pairs):\n",
        "        for p in pair:\n",
        "            f.write(f\"{p[0]},{p[1]},{p[2]},{p[3]}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Gx6FdiIfUDBd",
        "outputId": "25e10f16-4930-49a0-9ef9-92b541c277bd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HEAD | LEMMA | DEP | POS\n",
            "('recette', 'pot-au-feu', 'nsubj', 'NOUN')\n",
            "('pot-au-feu', 'inv', 'appos', 'NOUN')\n",
            "('recette', 'cuisine', 'nmod', 'NOUN')\n",
            "('cuisine', 'traditionnel', 'amod', 'ADJ')\n",
            "('cuisine', 'emblématique', 'amod', 'ADJ')\n",
            "('recette', 'historique', 'amod', 'NOUN')\n",
            "('cuisine', 'français', 'amod', 'ADJ')\n",
            "('repas', 'gastronomique', 'amod', 'ADJ')\n",
            "('repas', 'français', 'nmod', 'NOUN')\n",
            "('repas', 'base', 'nmod', 'NOUN')\n",
            "('base', 'viande', 'nmod', 'NOUN')\n",
            "('viande', 'bœuf', 'nmod', 'NOUN')\n",
            "('base', 'cuire', 'acl', 'VERB')\n",
            "('feu', 'doux', 'amod', 'ADJ')\n",
            "('repas', 'bouillon', 'nmod', 'NOUN')\n",
            "('bouillon', 'légume', 'nmod', 'NOUN')\n",
            "('repas', 'poireau', 'appos', 'NOUN')\n",
            "('repas', 'navet', 'appos', 'NOUN')\n",
            "('bouquet', 'garni', 'amod', 'ADJ')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(PATH_PAIRS, \"r\", encoding=\"utf-8\") as f:\n",
        "#     candidate_pairs = [tuple(line.strip().split(\",\"))\n",
        "#                        for line in f\n",
        "#                        if line.strip()]\n",
        "# print(f\"Loaded {len(candidate_pairs)} candidate pairs\")"
      ],
      "metadata": {
        "id": "yc01qm8dKnVI"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install numpy==1.24.3 --force-reinstall # Downgrade numpy to a version compatible with gensim\n",
        "!pip install --upgrade gensim --force-reinstall # Reinstall gensim with the compatible numpy version\n",
        "# !pip install --upgrade scipy --force-reinstall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zq4_-AUNQoko",
        "outputId": "697a260f-d5ce-41de-edf3-3fb979cdd168"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Using cached scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Collecting smart-open>=1.8.1 (from gensim)\n",
            "  Using cached smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
            "  Using cached wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Using cached scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "Using cached smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
            "Using cached wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
            "Installing collected packages: wrapt, numpy, smart-open, scipy, gensim\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 7.1.0\n",
            "    Uninstalling smart-open-7.1.0:\n",
            "      Successfully uninstalled smart-open-7.1.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 4.3.3\n",
            "    Uninstalling gensim-4.3.3:\n",
            "      Successfully uninstalled gensim-4.3.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1 smart-open-7.1.0 wrapt-1.17.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this if model is not downloaded\n",
        "!wget -c \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.vec.gz\" -O \"cc.fr.300.vec.gz\""
      ],
      "metadata": {
        "id": "GFAGmfi7RydO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24562fac-a7a0-48bf-e982-1f182522a7db"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-12 15:41:40--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.238.176.44, 18.238.176.19, 18.238.176.115, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.238.176.44|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1287757366 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.fr.300.vec.gz’\n",
            "\n",
            "cc.fr.300.vec.gz     26%[====>               ] 322.68M   114MB/s               ^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "from numpy.linalg import norm\n",
        "from collections import defaultdict\n",
        "\n",
        "model = KeyedVectors.load_word2vec_format('cc.fr.300.vec.gz', binary=False, limit=50000)  # Limit to 50k for memory\n",
        "\n",
        "# 1. Define your gastronomy “pivot” words\n",
        "pivots = [\"cuisine\",\"recette\",\"ingrédient\",\"épice\",\"gastronomie\"]\n",
        "# Keep only those that actually exist in the model\n",
        "pivot_vecs = [model[p] for p in pivots if p in model]\n",
        "if not pivot_vecs:\n",
        "    raise ValueError(\"None of your pivots were in the model!\")\n",
        "\n",
        "# 2. Compute the domain centroid\n",
        "domain_centroid = np.mean(pivot_vecs, axis=0)\n",
        "\n",
        "\n",
        "# 3. Load your candidate pairs (head, dependent)\n",
        "#    For example, from your pairs.txt:\n",
        "pairs = []\n",
        "with open(PATH_PAIRS, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        h, lemma, dep, pos = line.strip().split(\",\")\n",
        "        pairs.append((h, lemma, dep, pos))\n",
        "\n",
        "\n",
        "# 4. Define helper functions\n",
        "def pair_vector(h, lemma):\n",
        "    \"\"\"Average the two word vectors.\"\"\"\n",
        "    return (model[h] + model[lemma]) / 2\n",
        "\n",
        "def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    return float(a.dot(b) / (norm(a) * norm(b)))\n",
        "\n",
        "\n",
        "# 5. Compute similarities and filter\n",
        "THRESHOLD = 0.25\n",
        "filtered = []\n",
        "under_threshold = []\n",
        "for h, lemma, dep, pos in pairs:\n",
        "    if h in model and lemma in model:\n",
        "        vec = pair_vector(h, lemma)\n",
        "        sim = cosine_sim(domain_centroid, vec)\n",
        "        if sim >= THRESHOLD:\n",
        "            filtered.append((h, lemma, dep, pos, sim))\n",
        "        else:\n",
        "            under_threshold.append((h, lemma, dep, pos, sim))"
      ],
      "metadata": {
        "id": "i68d4CjMVL3n"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(f\"Under threshold - removed {len(under_threshold)} pairs:\")\n",
        "# for h, lemma, dep, pos, sim in under_threshold:\n",
        "#     print(f\"  {h:12s} {lemma:12s} → sim={sim:.3f}\")\n",
        "\n",
        "# print(\"\\n---------------------\\n\")\n",
        "\n",
        "# print(f\"Kept {len(filtered)}/{len(pairs)} pairs:\")\n",
        "# for h, lemma, dep, pos, sim in filtered:\n",
        "#     print(f\"  {h:12s} {lemma:12s} → sim={sim:.3f}\")\n"
      ],
      "metadata": {
        "id": "bJXhuruAV4dU",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save unique pairs to filtered_pairs.txt\n",
        "seen_pairs = set()  # Keep track of pairs we've already written\n",
        "\n",
        "with open(PATH_FILTERED_PAIRS, \"w\") as f:\n",
        "    for h, lemma, dep, pos, sim in filtered:\n",
        "        pair = (h, lemma)\n",
        "        if pair not in seen_pairs:\n",
        "            f.write(f\"{h},{lemma},{dep},{pos},{sim}\\n\")\n",
        "            seen_pairs.add(pair)\n",
        "\n",
        "with open(PATH_UNDER_PAIRS, \"w\") as f:\n",
        "    for h, lemma, dep, pos, sim in under_threshold:\n",
        "        pair = (h, lemma)\n",
        "        f.write(f\"{h},{lemma},{dep},{pos},{sim}\\n\")"
      ],
      "metadata": {
        "id": "JB2vRML95z7B"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests, csv, json, time\n",
        "from tqdm import tqdm  # Import tqdm for the progress bar\n",
        "\n",
        "BASE_URL = \"https://jdm-api.demo.lirmm.fr/v0/relations\"\n",
        "INPUT    = PATH_FILTERED_PAIRS # filtered_pairs.txt\n",
        "OUTPUT   = PATH_RELATIONS_RESULTS # relations_results.csv\n",
        "\n",
        "# Count the total number of lines in the input file\n",
        "with open(INPUT, encoding=\"utf-8\") as fin:\n",
        "    total_lines = sum(1 for line in fin)\n",
        "\n",
        "with open(INPUT, encoding=\"utf-8\") as fin, \\\n",
        "     open(OUTPUT, \"w\", newline=\"\", encoding=\"utf-8\") as fout:\n",
        "\n",
        "    writer = csv.DictWriter(fout, fieldnames=[\"node1\",\"node2\",\"relations\"])\n",
        "    writer.writeheader()\n",
        "\n",
        "    # Wrap the loop with tqdm to create a progress bar\n",
        "    for line in tqdm(fin, total=total_lines, desc=\"Processing pairs\"):\n",
        "        head, lemma, dep, pos, sim = line.strip().split(\",\")\n",
        "        resp = requests.get(f\"{BASE_URL}/from/{head}/to/{lemma}\")\n",
        "        resp.raise_for_status()\n",
        "        rels = resp.json().get(\"relations\", [])\n",
        "        writer.writerow({\n",
        "            \"node1\": head,\n",
        "            \"node2\": lemma,\n",
        "            \"relations\": json.dumps(rels, ensure_ascii=False),\n",
        "        })\n",
        "        time.sleep(0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9rJ7pUb_mTbQ",
        "outputId": "886ffd3c-6fe2-427f-eb9e-d336181eccc9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing pairs:   1%|          | 2/180 [00:01<02:07,  1.40it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-9c3a2324fc75>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Processing pairs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlemma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{BASE_URL}/from/{head}/to/{lemma}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mrels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"relations\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0;31m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0mserver_hostname_rm_dot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m             sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n\u001b[0m\u001b[1;32m    742\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m                 \u001b[0mcert_reqs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcert_reqs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_ssl_wrap_socket_and_match_hostname\u001b[0;34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[0m\n\u001b[1;32m    918\u001b[0m             \u001b[0mserver_hostname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m     ssl_sock = ssl_wrap_socket(\n\u001b[0m\u001b[1;32m    921\u001b[0m         \u001b[0msock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0mkeyfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpn_protocols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mALPN_PROTOCOLS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m     \u001b[0mssl_sock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ssl_wrap_socket_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtls_in_tls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mssl_sock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mSSLTransport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssl_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mssl_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;31m# SSLSocket class handles server_hostname encoding before it calls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0;31m# ctx._wrap_socket()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         return self.sslsocket_class._create(\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0msock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0mserver_side\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_side\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36m_create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1102\u001b[0m                         \u001b[0;31m# non-blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preview results**"
      ],
      "metadata": {
        "id": "7xlcLQ9or9FJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Load the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv(PATH_RELATIONS_RESULTS)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ─ Assumes you already have:\n",
        "#    df         = your DataFrame with columns ['node1','node2','relations']\n",
        "#    rel_types  = list of dicts loaded from relations_types.json\n",
        "\n",
        "# 1. Parse the JSON strings in 'relations'\n",
        "df['relations'] = df['relations'].apply(json.loads)\n",
        "\n",
        "# Load the CSV and JSON metadata\n",
        "with open(PATH_RELATIONS_TYPES, 'r', encoding='utf-8') as f:\n",
        "    rel_types = json.load(f)\n",
        "\n",
        "# 2. Explode & normalize into a flat DataFrame\n",
        "rel_df = pd.json_normalize(\n",
        "    df.explode('relations')['relations']\n",
        ").rename(columns={'type': 'rel_type', 'w': 'weight'})\n",
        "\n",
        "# 3. Compute aggregate stats\n",
        "stats = (\n",
        "    rel_df\n",
        "    .groupby('rel_type', as_index=False)\n",
        "    .agg(Count=('rel_type', 'size'),\n",
        "         Mean_w=('weight', 'mean'))\n",
        ")\n",
        "stats['% of total'] = stats['Count'] / stats['Count'].sum() * 100\n",
        "\n",
        "# 4. Build metadata DataFrame from rel_types\n",
        "meta_df = (\n",
        "    pd.DataFrame(rel_types)[['id','name','gpname']]\n",
        "    .rename(columns={'id':'rel_type','name':'Name','gpname':'Label'})\n",
        ")\n",
        "\n",
        "# 5. Merge, format, and select top 10\n",
        "merged = stats.merge(meta_df, on='rel_type').sort_values('Count', ascending=False)\n",
        "merged['Mean_w']     = merged['Mean_w'].round(2)\n",
        "merged['% of total'] = merged['% of total'].round(1)\n",
        "top10 = merged.head(10)[['rel_type','Name','Label','Count','% of total','Mean_w']]\n",
        "\n",
        "# 6. Display results\n",
        "print(top10.to_string(index=False))\n",
        "\n",
        "# 7. Visualize\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.bar(top10['Name'], top10['Count'])\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.ylabel('Nombre de relations')\n",
        "plt.title('Top 10 des types de relations par fréquence')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BZxUqkbPrPAQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}